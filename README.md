# Theoretical Principles of Deep Learning

Repository containing the final project of the Centrale Supélec course ‘Principes Théoriques de l’Apprentissage Profond’. 


## Description

This notebook contains an implementation of the experiments described in the paper 'Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data' (2023). This paper explores theoretical findings on two-layer Neural Network with both ReLU and leaky ReLU activation functions as well as experiments on synthetic data. This notebook implements all examples as described in the paper.

## Getting Started

### Requirements

* Python 3.13 (recommended)
* numpy
* torch
* torchvision
* sklearn
* matplotlib

You can install all the necesseray libraries using the following command :

```
pip install -r requirements.txt
```

### Project Structure

- Final_project.ipynb
- README.md
- requirements.txt

### Usage

Notebook can be ran as is. 

## Authors


* [CAUDARD Joris](https://github.com/JorisCaudard)